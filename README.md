# Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models (NaviBridger)

> ğŸ† Accepted at **CVPR 2025**  
> ğŸ”— [Github](https://github.com/hren20/NaiviBridger) | [arXiv](https://arxiv.org/abs/2504.10041)

<p align="center">
  <img src="assets/pipline.png" alt="Overview" width="80%">
</p>

---

## ğŸ“Œ TLDR

NaviBridger is a novel framework for visual navigation built upon **Denoising Diffusion Bridge Models (DDBMs)**. Unlike traditional diffusion policies that start from Gaussian noise, NaviBridger leverages **prior actions** (rule-based or learned) to guide the denoising process, accelerating convergence and improving trajectory accuracy.

---

## ğŸ› ï¸ Key Features
- ğŸ”§ DDBM-based policy generation from arbitrary priors  
- ğŸ” Unified framework supporting Gaussian, rule-based, and learning-based priors  
- ğŸƒâ€â™‚ï¸ Real-world deployment support on mobile robots (e.g., Diablo + Jetson Orin AGX)

---

## âœ… TODO List

- [ ] Deployment code updates
- [ ] A refactored version of the code (in the coming weeks)

---

## ğŸ“ Directory Overview

```
navibridge/
â”œâ”€â”€ train/                           # Training code and dataset processing
â”‚   â”œâ”€â”€ vint_train/                 # NaviBridger models, configs, and datasets
â”‚   â”œâ”€â”€ train.py                   # Training entry point
â”‚   â”œâ”€â”€ process_*.py              # Data preprocessing scripts
â”‚   â””â”€â”€ train_environment.yml     # Conda setup for training
â”œâ”€â”€ deployment/                     # Inference and deployment
â”‚   â”œâ”€â”€ src/navibridger_inference.py
â”‚   â”œâ”€â”€ config/params.yaml        # Inference config
â”‚   â”œâ”€â”€ deployment_environment.yaml
â”‚   â””â”€â”€ model_weights/            # Place for .pth model weights
â””â”€â”€ README.md                      # This file
```

---

## âš™ï¸ Setup

### ğŸ§ª Environment (Training)

```bash
conda env create -f train/train_environment.yml
conda activate navibridge_train
pip install -e train/
git clone git@github.com:real-stanford/diffusion_policy.git
pip install -e diffusion_policy/
```

### ğŸ’» Environment (Deployment)

```bash
conda env create -f deployment/deployment_environment.yaml
conda activate navibridge
pip install -e train/
pip install -e diffusion_policy/
```

---

## ğŸ“¦ Data Preparation

1. Download public datasets:
   - [RECON](https://sites.google.com/view/recon-robot/dataset)
   - [SCAND](https://www.cs.utexas.edu/~xiao/SCAND/)
   - [GoStanford2](https://cvgl.stanford.edu/gonet/dataset/)
   - [SACSoN](https://sites.google.com/view/sacson-review/huron-dataset)

2. Process datasets:
   ```bash
   python train/process_recon.py  # or process_bags.py
   python train/data_split.py --dataset <your_dataset_path>
   ```

3. Expected format:
```
dataset_name/
â”œâ”€â”€ traj1/
â”‚   â”œâ”€â”€ 0.jpg ... T_1.jpg
â”‚   â””â”€â”€ traj_data.pkl
â””â”€â”€ ...
```

After `data_split.py`, you should have:
```
train/vint_train/data/data_splits/
â””â”€â”€ <dataset_name>/
    â”œâ”€â”€ train/traj_names.txt
    â””â”€â”€ test/traj_names.txt
```

---

## ğŸ§  Model Training

```bash
cd train/
python train.py -c config/navibridge.yaml  # Select the training type by changing prior_policy
```

---

For learning-based method, training CVAE first:
```bash
python train.py -c config/cvae.yaml
```

---

## ğŸš€ Inference Demo

1. Place your trained model and config in:

```
deployment/model_weights/*.pth
deployment/model_weights/*.yaml
```
2. Adjust model path `deplyment/config/models.yaml`
3. Prepare input images (minimum 4): `0.png`, `1.png`, etc.  
   Adjust input directory path in `deployment/config/params.yaml`.

4. Run:

```bash
python deployment/src/navibridger_inference.py --model navibridge_cvae # Model name corresponding to key value in deplyment/config/models.yaml
```

---

## ğŸ¤– Hardware Tested
Here is our deployment platform information, you can replace it at will.

- NVIDIA Jetson Orin AGX  
- Intel RealSense D435i  
- Diablo wheeled-legged robot

> ğŸ“¸ RGB-only input, no depth or LiDAR required.

---

## ğŸ§ª Citation

```bibtex
@inproceedings{ren2025prior,
  title={Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models},
  author={Ren, Hao and Zeng, Yiming and Bi, Zetong and Wan, Zhaoliang and Huang, Junlong and Cheng, Hui},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}
```

---

## ğŸ“œ License

This codebase is released under the [MIT License](LICENSE).

## Acknowledgment
NaviBridger is inspired by the contributions of the following works to the open-source community: [DDBM](https://github.com/alexzhou907/DDBM), [NoMaD](https://github.com/robodhruv/visualnav-transformer), and [BRIDGER](https://github.com/clear-nus/bridger). We thank the authors for sharing their outstanding work.